# QMD Improvements Port â€” Roadmap

**Goal:** Port QMD's search quality, query expansion, reranking, and MCP support to our knowledge-graph.

**Reference:** https://github.com/tobi/qmd (Tobi LÃ¼tke's project)

---

## Current State

| Feature | QMD | Our KG | Gap |
|---------|-----|--------|-----|
| Vector Search | âœ… embeddinggemma | âœ… Ollama nomic-embed | âœ… |
| BM25 (FTS) | âœ… SQLite FTS5 | âŒ | Need to add |
| Hybrid Fusion | âœ… RRF | âŒ | Need to add |
| Query Expansion | âœ… Fine-tuned 1.7B | âŒ | Need to add |
| Reranking | âœ… qwen3-reranker | âŒ | Need to add |
| RAG Answers | âŒ | âœ… Claude | âœ… (we're ahead) |
| MCP Server | âœ… | âŒ | Need to add |
| HTTP API | âŒ | âœ… FastAPI | âœ… (we're ahead) |
| n8n/Webhook | âŒ | âœ… | âœ… (we're ahead) |

---

## Phase 1: Hybrid Search (BM25 + Vector + RRF)

**Impact:** High â€” biggest search quality improvement

### 1.1 Add SQLite FTS5 Table

```python
# New file: fts_index.py
import sqlite3

class FTSIndex:
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        self._init_tables()
    
    def _init_tables(self):
        self.conn.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts 
            USING fts5(
                file_path,
                title,
                content,
                tokenize='porter unicode61'
            )
        """)
    
    def search(self, query: str, limit: int = 30) -> List[dict]:
        cursor = self.conn.execute("""
            SELECT file_path, title, snippet(documents_fts, 2, '<b>', '</b>', '...', 64),
                   bm25(documents_fts) as score
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY score
            LIMIT ?
        """, (query, limit))
        return [{"file_path": r[0], "title": r[1], "snippet": r[2], "score": abs(r[3])} 
                for r in cursor.fetchall()]
```

### 1.2 Implement Reciprocal Rank Fusion (RRF)

```python
def reciprocal_rank_fusion(result_lists: List[List[dict]], k: int = 60) -> List[dict]:
    """
    Combine multiple ranked lists using RRF.
    
    RRF score = Î£ 1/(k + rank) for each list the doc appears in
    
    k=60 is standard (balances high vs low ranked docs)
    """
    scores = {}
    docs = {}
    
    for results in result_lists:
        for rank, doc in enumerate(results):
            doc_id = doc["file_path"]
            if doc_id not in scores:
                scores[doc_id] = 0
                docs[doc_id] = doc
            scores[doc_id] += 1.0 / (k + rank + 1)
    
    # Sort by fused score
    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    return [{"score": score, **docs[doc_id]} for doc_id, score in sorted_docs]
```

### 1.3 Integrate into Searcher

```python
async def hybrid_search(self, query: str, vault: str, limit: int = 10):
    # Run BM25 and vector search in parallel
    bm25_results = self.fts_index.search(query, limit=30)
    vector_results = await self.vector_search(query, vault, limit=30)
    
    # Fuse results
    fused = reciprocal_rank_fusion([bm25_results, vector_results])
    
    return fused[:limit]
```

---

## Phase 2: Query Expansion

**Impact:** Medium â€” helps with imprecise queries

### 2.1 Add Query Expansion via Ollama

```python
QUERY_EXPANSION_PROMPT = """Generate 2 alternative search queries for: "{query}"

Rules:
- Keep semantic meaning
- Use different words/phrasings
- One should be more specific, one more general

Output format (exactly):
1. [first alternative]
2. [second alternative]"""

async def expand_query(self, query: str) -> List[str]:
    response = await self.ollama_generate(QUERY_EXPANSION_PROMPT.format(query=query))
    # Parse alternatives
    alternatives = parse_numbered_list(response)
    return [query] + alternatives  # Original + expansions
```

### 2.2 Multi-Query Search

```python
async def expanded_hybrid_search(self, query: str, vault: str, limit: int = 10):
    queries = await self.expand_query(query)
    
    all_results = []
    for q in queries:
        results = await self.hybrid_search(q, vault, limit=30)
        all_results.append(results)
    
    # Weight original query higher (appears twice)
    all_results.insert(0, all_results[0])  # Double-weight original
    
    fused = reciprocal_rank_fusion(all_results)
    return fused[:limit]
```

---

## Phase 3: LLM Reranking

**Impact:** High â€” significant quality boost for top results

### 3.1 Add Reranker

QMD uses `qwen3-reranker-0.6b`. We can use Ollama with a similar approach:

```python
RERANK_PROMPT = """Query: {query}

Document: {document}

Is this document relevant to the query? Answer only YES or NO."""

async def rerank(self, query: str, documents: List[dict], top_k: int = 10) -> List[dict]:
    scored = []
    
    for doc in documents[:30]:  # Rerank top 30
        response = await self.ollama_generate(
            RERANK_PROMPT.format(query=query, document=doc["content"][:2000]),
            model="qwen2.5:0.5b"  # Small, fast model
        )
        
        # Score based on YES/NO
        is_relevant = response.strip().upper().startswith("YES")
        score = 1.0 if is_relevant else 0.0
        scored.append({**doc, "rerank_score": score})
    
    # Position-aware blending (QMD approach)
    for i, doc in enumerate(scored):
        rrf_weight = 0.75 if i < 3 else (0.60 if i < 10 else 0.40)
        rerank_weight = 1 - rrf_weight
        doc["final_score"] = (rrf_weight * doc["score"]) + (rerank_weight * doc["rerank_score"])
    
    return sorted(scored, key=lambda x: x["final_score"], reverse=True)[:top_k]
```

---

## Phase 4: MCP Server

**Impact:** High â€” enables Claude Desktop/Code integration

### 4.1 Add MCP Endpoint

```python
# New file: mcp_server.py
from mcp import Server, Tool

class KnowledgeGraphMCP:
    def __init__(self, searcher):
        self.searcher = searcher
        self.server = Server("knowledge-graph")
        self._register_tools()
    
    def _register_tools(self):
        @self.server.tool("kg_search")
        async def search(query: str, vault: str = "all", limit: int = 10):
            """Fast hybrid search (BM25 + vector)"""
            return await self.searcher.hybrid_search(query, vault, limit)
        
        @self.server.tool("kg_query")
        async def query(question: str, vault: str = "all"):
            """Hybrid search + reranking + RAG answer"""
            return await self.searcher.query_with_llm(question, vault)
        
        @self.server.tool("kg_get")
        async def get_document(file_path: str):
            """Get full document by path"""
            return await self.searcher.get_document(file_path)
```

### 4.2 Run MCP Server

```bash
# Add CLI command
python -m knowledge_graph.mcp serve
```

### 4.3 Claude Desktop Config

```json
{
  "mcpServers": {
    "knowledge-graph": {
      "command": "python",
      "args": ["-m", "knowledge_graph.mcp", "serve"],
      "env": {
        "KG_API_URL": "http://192.168.1.70:8080"
      }
    }
  }
}
```

---

## Phase 5: OpenClaw Skill Enhancement

**Impact:** Medium â€” better agent integration

### 5.1 Update Skill with New Endpoints

```yaml
# SKILL.md updates
- Add /hybrid-search endpoint
- Add /query-expanded endpoint  
- Document reranking behavior
- Add MCP setup instructions
```

---

## Implementation Order

| Phase | Effort | Impact | Priority |
|-------|--------|--------|----------|
| 1. Hybrid Search | Medium | High | ðŸ”´ First |
| 3. Reranking | Medium | High | ðŸ”´ Second |
| 2. Query Expansion | Low | Medium | ðŸŸ¡ Third |
| 4. MCP Server | Medium | High | ðŸŸ¡ Fourth |
| 5. Skill Update | Low | Medium | ðŸŸ¢ Last |

---

## Files to Create/Modify

```
services/api/
â”œâ”€â”€ fts_index.py       # NEW - SQLite FTS5 wrapper
â”œâ”€â”€ reranker.py        # NEW - LLM reranking
â”œâ”€â”€ query_expander.py  # NEW - Query expansion
â”œâ”€â”€ fusion.py          # NEW - RRF implementation
â”œâ”€â”€ mcp_server.py      # NEW - MCP protocol
â”œâ”€â”€ searcher.py        # MODIFY - integrate hybrid search
â”œâ”€â”€ main.py            # MODIFY - add new endpoints
â””â”€â”€ requirements.txt   # MODIFY - add mcp-sdk
```

---

## Models Needed (Ollama)

| Model | Purpose | Size |
|-------|---------|------|
| nomic-embed-text | Embeddings (existing) | ~275MB |
| qwen2.5:0.5b | Reranking + expansion | ~400MB |

Or use dedicated reranker: `ollama pull snowflake-arctic-embed-rerank`

---

## Next Steps

1. âœ… Create this roadmap
2. â³ Implement Phase 1 (Hybrid Search)
3. â³ Test with existing data
4. â³ Implement Phase 3 (Reranking)
5. â³ Add MCP server
6. â³ Update skill

---

*Created: 2026-02-06*
*Reference: github.com/tobi/qmd*
